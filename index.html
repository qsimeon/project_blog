<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>
    <title>Aligning Modalities: Efficient Multimodal Representation Learning</title>
</head>

<body>
    <header>
        <h1>Towards the Platonic Representation via Multimodal Contrastive Alignment</h1>
    </header>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>
            <p>
                Deep learning models are typically trained to transform raw data into representations optimized for
                specific tasks. Recently, two lines of research have inspired a deeper inquiry into the nature of these
                representations. The CLIP framework demonstrated the utility of aligning representations across
                modalities, using paired image-text data to train joint embeddings for cross-modal retrieval. Meanwhile,
                the Platonic Representation Hypothesis posits that performant models converge toward a shared
                statistical model of reality in their representation spaces, suggesting a potential universality
                underlying learned representations.
            </p>
            <p>
                This project bridges these ideas by exploring whether representations from disparate pre-trained
                unimodal neural networks can be aligned into a shared multimodal latent space, inspired by the joint
                embedding approach of CLIP and motivated by the convergence hypothesis of Platonic Representations. The
                proposed framework uses frozen unimodal encoders with learned linear adapters to align representations
                across modalities. Our aim is to determine if such aligned representations better approximate those of
                larger, more performant models.
            </p>
            <p>
                Inspired by the success of CLIP in aligning representations across modalities and the theoretical
                insights of the Platonic Representation Hypothesis, we propose a framework that aligns pre-trained
                unimodal encoders into a shared multimodal latent space using simple linear adapters. By leveraging
                frozen encoders such as ResNet-18 for images and DistilBERT for text, we aim to achieve alignment
                without retraining a larger state-of-the-art model like DINOv2, enabling scalability to new modalities
                with minimal overhead.
            </p>
            <figure>
                <img src="figures/conceptual_contrastive.svg" alt="Conceptual Multimodal Alignment"
                    style="width: 100%; height: auto;">
                <figcaption>
                    <strong>Figure 1.</strong> Conceptual illustration of our multimodal alignment framework. Image
                    representations (from ResNet-18) and text representations (from DistilBERT) for images and
                    corresponding first captions sampled from the Flickr30k dataset are aligned into a
                    shared latent space using lightweight adapters. Matching image-caption pairs are pulled together,
                    while non-matching pairs are pushed apart, approximating a unified embedding inspired by the
                    Platonic Representation Hypothesis.
                </figcaption>
            </figure>


            <h3>Motivation</h3>
            <p>
                Our approach is motivated by three key insights:
            </p>
            <ol>
                <li><strong>Inspiration from CLIP:</strong> The CLIP framework demonstrated that cross-modal
                    representations could be aligned through paired data and contrastive learning. However, it requires
                    joint training of encoders, limiting extensibility to additional modalities. Our method decouples
                    the encoders, focusing instead on aligning their outputs via lightweight adapters.</li>
                <li><strong>Testing the Platonic Representation Hypothesis:</strong> This hypothesis posits that
                    performant models converge toward a shared statistical model of reality in their representation
                    spaces. By aligning diverse unimodal encoders, we provide a testbed for exploring whether this
                    convergence can be achieved explicitly.</li>
                <li><strong>Scalability and Modularity:</strong> Traditional multimodal models often require joint
                    training on extensive datasets. By aligning frozen encoders post hoc, our framework supports modular
                    integration of new modalities, enabling efficient experimentation.</li>
            </ol>


        </section>

        <section id="related-work">
            <h2>Related Work</h2>
            <p>
                Our project builds on several important advances in multimodal representation learning and theoretical
                insights into model convergence:
            </p>
            <ol>
                <li>
                    <strong><a href="https://phillipi.github.io/prh/">Platonic Representation Hypothesis
                            (PRH)</a>:</strong> <a href="https://arxiv.org/abs/2405.07987">Huh et al. (2024)</a>
                    proposed that
                    performant models converge toward a shared statistical model of reality in their representation
                    spaces,
                    regardless of their training modality. This hypothesis underpins our framework, which aims to
                    explicitly align unimodal encoders to test this convergence hypothesis.
                </li>
                <li>
                    <strong><a href="https://openai.com/index/clip/">CLIP</a>:</strong> <a
                        href="https://arxiv.org/abs/2103.00020">Radford et al. (2021) </a>introduced CLIP, a model that
                    learns joint multimodal
                    representations using contrastive learning on paired image-text datasets. CLIP's success
                    demonstrates the power of cross-modal embeddings but requires joint training of encoders, which our
                    framework aims to circumvent.
                </li>
                <li>
                    <strong>Linear Mapping from Image to Text Space:</strong> <a
                        href="https://arxiv.org/abs/2209.15162">Merullo et al. (2022)</a> showed that
                    simple linear transformations can align visual representations to text spaces, enabling cross-modal
                    tasks
                    like visual question answering. Their findings inspired our use of linear and lightweight MLP
                    adapters for multimodal alignment.
                </li>
                <li>
                    <strong>DINOv2:</strong> <a href="https://arxiv.org/abs/2304.07193">Oquab et al. (2023)</a>
                    introduced DINOv2, a self-supervised vision transformer
                    model that generates robust embeddings. As our performant reference model, we use DINOv2-small to
                    provide a benchmark for evaluating the quality of our aligned multimodal representations.
                </li>
                <li>
                    <strong>Contrastive Representation Learning:</strong>
                    <a href="https://arxiv.org/abs/2005.10242">Wang and Isola (2020)</a> provided theoretical
                    insights into
                    contrastive learning by analyzing alignment and uniformity on the hypersphere. Their work shows that
                    contrastive
                    objectives promote both alignment of positive pairs and uniformity of representations across the
                    hypersphere.
                    This analysis informs our use of the <strong>dual-encoder contrastive loss</strong> to drive
                    convergence in the shared latent space.
                </li>
            </ol>
        </section>

        <section id="hypothesis">
            <h2>Hypothesis</h2>
            <p>
                Our work is grounded in the following assumptions and/or hypotheses:
            </p>
            <ul>
                <li>A shared latent space exists where unimodal representations from different encoders can be aligned
                    through linear transformations.</li>
                <li>Aligning these representations produces embeddings that closely approximate those of performant
                    models, such as DINOv2, as measured by kernel alignment metrics.</li>
                <li>Multimodal alignment captures mechanisms of representation convergence, offering empirical evidence
                    for the Platonic Representation Hypothesis.</li>
            </ul>
        </section>


        <section id="mathematical-framework">
            <h2>Mathematical Framework</h2>
            <p>
                At the core of our project is the hypothesis that pre-trained unimodal representations can be aligned
                into a shared multimodal latent space. This section formalizes the mathematical structure underlying our
                framework, detailing how representations are extracted, aligned, and evaluated.
            </p>

            <h3>Multimodal Data Representation</h3>
            <p>
                Let the world generate raw multimodal data:
            </p>
            <p>
                \[
                \mathcal{D}_\text{world} = \left\{\left(\theta^{(i)}, \psi^{(i)}, \phi^{(i)}, \dots \right)
                \right\}_{i=1}^N, \quad \theta \in \mathcal{\Theta}, \psi \in \mathcal{\Psi}, \phi \in \mathcal{\Phi},
                \dots
                \]
            </p>
            <p>
                where \( \mathcal{\Theta}, \mathcal{\Psi}, \mathcal{\Phi} \) represent different modalities (e.g.,
                image, text, audio), and \( \theta, \psi, \phi \) are specific instances of these modalities.
            </p>
            <p>
                For simplicity, we focus on a two-modality setting with images (\( \mathcal{X} \)) and text (\(
                \mathcal{Y} \)):
            </p>
            <p>
                \[
                \mathcal{D}_\text{world} = \left\{\left(x^{(i)}, y^{(i)}\right)\right\}_{i=1}^N, \quad x \in
                \mathcal{X}, y \in \mathcal{Y}.
                \]
            </p>
            <figure>
                <img src="figures/platonic_representation.svg" alt="Platonic Representation Hypothesis"
                    style="width: 80%; height: auto;">
                <figcaption>
                    <strong>Figure 2.</strong> Conceptual illustration of the connection between the Platonic
                    Representation Hypothesis (PRH) and our project framework, adapted from Figure 1 of PRH blog.
                    Representations from different modalities (e.g., images, text) are hypothesized to converge toward a
                    shared statistical model — the Platonic representation. Our framework uses contrastive learning to
                    explicitly align these representations into a shared latent space, approximating this convergence.
                </figcaption>
            </figure>


            <h3>Learned Adapters</h3>
            <p>
                We use frozen pre-trained encoders to extract representations from each modality:
            </p>
            <p>
                \[
                X_\text{enc}: \mathcal{X} \rightarrow \mathbb{R}^{d_x}, \quad Y_\text{enc}: \mathcal{Y} \rightarrow
                \mathbb{R}^{d_y},
                \]
            </p>
            <p>
                where \( d_x \) and \( d_y \) are the embedding dimensions for images and text, respectively.
            </p>
            <p>
                We use a pretrained <a href="https://huggingface.co/microsoft/resnet-18">ResNet-18
                    model pretrained</a> on <a href="https://www.image-net.org/download.php">ImageNet1K</a> as our
                frozen pretrained imaged encoder and a <a
                    href="https://huggingface.co/distilbert/distilbert-base-uncased">DistilBERT model pretrained</a> on
                <a href="https://yknzhu.wixsite.com/mbweb">BookCorpus</a> as our frozen text encoder.

            </p>
            <p>
                To project these representations into a shared latent space \( \mathbb{R}^{d_e} \), we introduce learned
                adapters:
            </p>
            <p>
                \[
                W_x : \mathbb{R}^{d_x} \rightarrow \mathbb{R}^{d_e}, \quad W_y : \mathbb{R}^{d_y} \rightarrow
                \mathbb{R}^{d_e}
                \]
            </p>
            <p>
                We explore both <strong>linear adapters</strong> and <strong>2-layer MLP adapters</strong>. The adapted
                encoders:
            </p>
            <p>
                \[
                \begin{aligned}
                f_\text{image}: \mathcal{X} \rightarrow \mathbb{R}^{d_e}, \quad f_\text{image} &= W_x \circ X_\text{enc}
                \\
                g_\text{text}: \mathcal{Y} \rightarrow \mathbb{R}^{d_e}, \quad g_\text{text} &= W_y \circ Y_\text{enc}
                \end{aligned}
                \]
            </p>
            <p>
                \( f_\text{image} \) and \( g_\text{text} \) map images and text, respectively, to the shared latent
                space \( \mathbb{R}^{d_e} \).
            </p>

            <h3>Dual-Encoder Contrastive Objective</h3>
            <p>
                To align the representations, we use a <strong>dual-encoder contrastive loss</strong>.
            <details>
                <summary>Homework 4</summary>
                <figure>
                    <img src="figures/homework4_contrastive.jpg" alt="" style="width: 80%; height: auto;">
                </figure>
            </details>
            </p>


            <p>
                Our loss encourages is designed to encourage
                representations from matching pairs to be similar while pushing apart representations from non-matching
                pairs:
            </p>
            <p>
                \[
                \mathcal{L}_{\text{contrastive}} = -\frac{1}{N} \sum_{i=1}^N \log
                \frac{\exp\left(f_\text{image}(x^{(i)})
                \cdot g_\text{text}(y^{(i)}) / \tau\right)}{\sum_{j=1}^N \exp\left(f_\text{image}(x^{(i)}) \cdot
                g_\text{text}(y^{(j)}) / \tau\right)},
                \]
            </p>
            <p>
                where \( \tau \) is the temperature parameter that scales the similarity scores.
            </p>
            <p>
                Our use of the dual-encoder contrastive objective is a unique extension of traditional contrastive
                learning. Typically, contrastive learning applies to different "views" generated through transformations
                or augmentations of samples from a <em>single modality</em>. In our approach, we generalize this concept
                by treating <em>different modalities</em> (image and text, in this case) as distinct views of a
                <strong>hypothesized common underlying reality</strong>, inspired by the Platonic Representation
                Hypothesis.
            </p>


            <h3>Connection to PRH</h3>
            <p>
                The <strong>Platonic Representation Hypothesis (PRH)</strong> posits that representations from different
                modalities converge toward a shared Platonic representation. In our framework, we hypothesize that
                <strong>contrastive alignment</strong> between the unimodal image \( f_{\text{image}} \) and text \(
                g_{\text{text}} \) encoders can drive this convergence toward the Platonic representation.
            </p>

            <p>
                In the ideal case of perfect contrastive alignment (achieved by minimizing the diameter and maximizing
                the margin), the outputs of \( f_{\text{image}}(x) \) and \( g_{\text{text}}(y) \) for positive
                pairs would map to the <strong>same embedding vector</strong> in the shared latent space i.e. \(
                f_{\text{image}}(x^{(i)})
                = g_{\text{text}}(y^{(i)}) \)
            </p>

            <p>
                Our thesis is that this limiting vector represents the Platonic representation. However, in practice,
                due to finite data, model capacity, and noise, the outputs of \( f_{\text{image}} \) and \(
                g_{\text{text}} \) are typically <strong>close</strong> (as measured by the \( L_2
                \)-distance on the unit hypersphere \( \mathbb{S}^{d_e-1} \)) but not identical i.e. \(
                f_{\text{image}}(x^{(i)})
                \approx g_{\text{text}}(y^{(i)}) \).
            </p>

            <p>
                Given this, we define our <strong>aligned multimodal encoder</strong> \( h_\text{multi} \) as a function
                that "averages" (i.e takes a convex combination of) the embeddings output by the adapted image and text
                encoders, i.e.:
            </p>
            <p>
                \[
                h_\text{multi}: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}^{d_e}, \quad h_\text{multi}(x, y)
                =
                \lambda \cdot f_{\text{image}}(x) + (1 - \lambda) \cdot g_{\text{text}}(y)
                \]
            </p>

            <p>
                where \( \lambda \in [0, 1] \) is a learnable hyperparameter that controls the balance between the image
                and text
                embeddings.

            </p>
            <p>
                We call the output \( h_\text{multi}(x^{(i)}, y^{(i)}) \) the <strong>aligned multimodal
                    representation</strong>
                and claim that this is an <strong>approximation</strong> of the Platonic representation of the \(i\)-th
                data point. While the true
                Platonic representation is unknown, we follow the PRH
                conjecture that performant models (such as <a
                    href="https://github.com/facebookresearch/dinov2">DINOv2</a>)
                produce representations that are converging toward it. We use the features extracted by a
                pretrained <a href="https://huggingface.co/facebook/dinov2-small">DINOv2-small</a> backbone as a
                <strong>proxy</strong> for the Platonic representation.
            </p>

            <h3>Performant Model Benchmark</h3>
            <p>
                We will refer to the function implemented by the DINOv2-small feature extractor as the
                DINOv2-small encoder:
            </p>
            <p>
                \[
                \text{DINOv2}: \mathcal{X} \rightarrow \mathbb{R}^{d_e}.
                \]
            </p>

            <p>
                This emphasizes its role as another image encoder that serves as a reference or benchmark for evaluating
                our aligned
                representations. We evaluate the quality of our aligned multimodal encoder \( h_\text{multi} \) its
                representations to
                those of the DINOv2-small encoder using a <strong>kernel alignment metric</strong>.
            </p>

            <h3>Kernel Alignment Metric</h3>
            <p>
                We characterize representations in terms of their <strong>kernels</strong>, which capture the similarity
                between data points. Two representations are considered <strong>aligned</strong> if their kernels are
                similar for corresponding inputs. For example, if the text encoder \( g_{\text{text}} \) is aligned with
                the image encoder \( f_{\text{image}} \), the similarity between text representations of "apple" and
                "orange" should correspond closely to the similarity between their image representations:
            </p>

            <figure>
                <img src="figures/similarity_metric.jpg" alt="Similarity Metric" style="width: 90%; height: auto;">
            </figure>

            <p>
                To quantify this alignment, we use the <strong>mutual \(k\)-nearest neighbor (mutual-KNN)</strong>
                kernel alignment metric \( m \), as introduced by <a href="https://arxiv.org/abs/2405.07987">Huh et al.
                    (2024)</a>. We define the kernels as:
            </p>

            <ul>
                <li>
                    <strong>Unimodal Kernels:</strong>
                    <p>
                        \[
                        K_X(i, j) = \langle f_{\text{image}}(x^{(i)}), f_{\text{image}}(x^{(j)}) \rangle, \quad K_Y(i,
                        j) =
                        \langle g_{\text{text}}(y^{(i)}), g_{\text{text}}(y^{(j)}) \rangle
                        \]
                    </p>
                </li>
                <li>
                    <strong>Aligned Multimodal Kernel:</strong>
                    <p>
                        \[
                        K_{\text{align}}(i, j) = \langle h_\text{multi}(x^{(i)}, y^{(i)}) , h_\text{multi}(x^{(j)},
                        y^{(j)}) \rangle,
                        \]
                    </p>

                </li>
                <li>
                    <strong>Performant Model Kernel:</strong>
                    <p>
                        \[
                        K_\text{DINOv2}(i, j) = \langle \text{DINOv2}(x^{(i)}), \text{DINOv2}(x^{(j)}) \rangle
                        \]
                    </p>
                </li>
            </ul>

            <h4>Before Training</h4>
            <p>
                We compute the alignment metrics for the unimodal kernels relative to the performant model kernel:
            </p>
            <p>
                \[
                m(K_X, K_{\text{DINOv2}}), \quad m(K_Y, K_{\text{DINOv2}}).
                \]
            </p>

            <h4>After Training</h4>
            <p>
                We evaluate the mutual-KNN alignment for the aligned multimodal kernel against the performant model
                kernel:
            </p>
            <p>
                \[
                m(K_{\text{align}}, K_{\text{DINOv2}}).
                \]
            </p>

            <h4>Key Hypothesis</h4>
            <p>
                We hypothesize that the aligned multimodal kernel achieves higher similarity with \( K_{\text{DINOv2}}
                \) than the average of the unimodal kernels:
            </p>
            <p>
                \[
                m(K_{\text{align}}, K_{\text{DINOv2}}) > \text{avg}\left(m(K_X, K_{\text{DINOv2}}), m(K_Y,
                K_{\text{DINOv2}})\right).
                \]
            </p>

            <p>
                A stronger version of this hypothesis posits that the aligned multimodal kernel surpasses even the best
                unimodal kernel:
            </p>
            <p>
                \[
                m(K_{\text{align}}, K_{\text{DINOv2}}) > \max\left(m(K_X, K_{\text{DINOv2}}), m(K_Y,
                K_{\text{DINOv2}})\right).
                \]
            </p>

            <p>
                This would suggest an interesting emergent-like property where the whole (multimodal representation) is
                greater than either its parts (unimodal representations) alone,
            </p>

        </section>

        <section id="methodology">
            <h2>Methodology</h2>

            <p>
                All code for this project is available in our
                <a href="https://colab.research.google.com/drive/1BkyPko0x-8CL41Z-VSmyxI41B90hB2Fc?usp=sharing">Google
                    Colab notebook</a>.
            </p>

            <h3>Data Pipeline</h3>
            <p>
                We construct our world dataset \( \mathcal{D}_\text{world} \) from the first 12,800 samples of the
                <strong><a href="https://huggingface.co/datasets/nlphuji/flickr30k">Flickr30k</a></strong> dataset,
                which contains paired image-caption samples. Each image in Flickr30k dataset is
                associated with 5 captions, but we use only the first caption so that we have a single postive pair per
                data point \( x^{(i)}, y^{(i)} \).
            </p>
            <ul>
                <li><strong>Image Processing:</strong>
                    <ul>
                        <li>Resize to 224x224.</li>
                        <li>Normalize using ImageNet statistics.</li>
                        <li>Convert to tensors for ResNet-18.</li>
                    </ul>
                </li>
                <li><strong>Text Processing:</strong>
                    <ul>
                        <li>Tokenize with DistilBERT tokenizer.</li>
                        <li>Pad/truncate to 64 tokens.</li>
                        <li>Convert to tensors for DistilBERT.</li>
                    </ul>
                </li>
                <li><strong>Data Loading:</strong>
                    <ul>
                        <li>Batch size: 256.</li>
                        <li>Train-validation split: 75%-25%.</li>
                    </ul>
                </li>
            </ul>
            <details>
                <summary><em>Expand for schematic of data pipeline.</em></summary>
                <img src="figures/data_pipeline.svg" alt="Data Pipeline">
            </details>

            <h3>Model Architecture</h3>
            <p>
                The architecture uses frozen pre-trained encoders and trainable adapters for aligning representations in
                a shared multimodal space.
            </p>
            <ul>
                <li><strong>Frozen Encoders:</strong>
                    <ul>
                        <li>Image: ResNet-18 (output dimension: \( d_x = 512\)).</li>
                        <li>Text: DistilBERT (output dimension: \( d_y = 768\)).</li>
                    </ul>
                </li>
                <li><strong>Adapters \( \left( W_x, W_y \right) \):</strong>
                    <ul>
                        <li>Linear Adapter: Matrix multiplication plus bias term.</li>
                        <li>MLP Adapter: 2-layer MLP with GELU and LayerNorm.</li>
                        <li>Output dimension: \( d_e = 768 \).</li>
                    </ul>
                </li>
                <li><strong>Reference Model:</strong> DINOv2-small (output dimension: \( d_e = 768 \)).</li>
            </ul>

            <h3>Training Procedure</h3>
            <p>
                We train the adapters to align image and text representations using the dual-encoder
                contrastive loss.
                Both <strong>linear </strong> and <strong>2-layer MLP</strong> adapters are trained for <strong>100
                    epochs</strong> on the <strong>Flickr30k</strong> dataset
                using a temperature scaling value of \( \tau = 0.02 \) in the contrastive loss. Other releavnt
                hyperparameter settings are:
            </p>

            <ul>
                <li><strong>Optimizer:</strong> AdamW</li>
                <li><strong>Learning Rate:</strong> \(1 \times 10^{-4}\) (cosine decay with warmup).</li>
                <li><strong>Gradient Clipping:</strong> 0.5.</li>
                <li><strong>Weight Decay:</strong> 0.001 (linear adapter) and 0.005 (MLP adapter).</li>
                <li><strong>Label Smoothing:</strong> 0.05.</li>
                <li><strong>Dropout:</strong> 0.1.</li>
            </ul>
            <details>
                <summary><em>Expand for schematic of training pipeline.</em></summary>
                <img src="figures/training_pipeline.svg" alt="Training Pipeline">
            </details>

            <h3>Evaluation Metrics</h3>
            <p>
                We evaluate the alignment performance for the two types of adapters
                using the <strong>mutual-KNN</strong> kernel alignment metric \( m \).
                Specifically, we compute the following three values:
            </p>
            <ul>
                <li><strong>Unimodal Alignment</strong>:</li>
                <ol>
                    <li value="1">\( m(K_X, K_{\text{DINOv2}}) \): Alignment of the image encoder \( f_\text{image} \)
                        kernel to
                        the
                        DINOv2 kernel.</li>
                    <li value="2">\( m(K_Y, K_{\text{DINOv2}}) \): Alignment of the text encoder \( g_\text{text} \)
                        kernel to the
                        DINOv2 kernel.</li>
                </ol>

                <li><strong>Multimodal Alignment</strong>:</li>
                <ol>
                    <li value="3">\( m(K_{\text{align}}, K_{\text{DINOv2}}) \): Alignment of the multimodal encoder \(
                        h_\text{mult}
                        \) kernel to the DINOv2 kernel.</li>
                </ol>
            </ul>

            <p>We use these values to test our two <strong>key hypotheses</strong>:
            </p>
            <ul>
                <li>
                    <strong>Weak Hypothesis \(H_1\):</strong> The aligned multimodal kernel achieves higher similarity
                    with
                    the DINOv2 kernel than the average of the unimodal kernels:
                    \[
                    m(K_{\text{align}}, K_{\text{DINOv2}}) > \text{avg}(m(K_X, K_{\text{DINOv2}}), m(K_Y,
                    K_{\text{DINOv2}})).
                    \]
                </li>
                <li>
                    <strong>Strong Hypothesis \(H_2\):</strong> The aligned multimodal kernel surpasses even the best
                    unimodal
                    kernel:
                    \[
                    m(K_{\text{align}}, K_{\text{DINOv2}}) > \max(m(K_X, K_{\text{DINOv2}}), m(K_Y,
                    K_{\text{DINOv2}})).
                    \]
                </li>
            </ul>
            <details>
                <summary><em>Expand for schematic of hypothesis testing.</em></summary>
                <img src="figures/hypothesis_test.svg" alt="Hypothesis Testing">
            </details>
        </section>


        <section id="results">
            <h2>Results</h2>

            <h3>Kernel Alignment Improves with Training</h4>
                <p>
                    Our first key finding is that the mutual-KNN kernel alignment metric improves
                    after training for both the linear and MLP adapters. This demonstrates that our
                    contrastive alignment objective effectively aligns the image and text representations.
                </p>

                <figure style="width: 100%; max-width: 1200px; margin: 0 auto;">
                    <img src="figures/kernel_metric_barplot.png" alt="Kernel Metric Bar Plot"
                        style="width: 100%; height: auto;">
                    <figcaption>
                        <strong>Figure 3.</strong> Bar plot showing the mutual-KNN kernel alignment metric before and
                        after training for the (A) linear and (B) MLP adapters. For both adapter types and
                        all encoders, kernel alignment to the DINOv2-small encoder improves after training with the
                        dual encoder contrastive objective.
                    </figcaption>
                </figure>

                <p>
                    We use PCA to visualize the alignment process between the DINOv2-small embeddings and the aligned
                    multimodal
                    embeddings. First, we apply the DINOv2-small encoder to our test set and compute the principal
                    components (PCs) of the resulting \(3200 \times 784\) matrix of embeddings. During training, at each
                    epoch,
                    we project the aligned multimodal embeddings onto the same PCs and plot the first two principal
                    component
                    scores for both sets of embeddings. This 2D visualization shows how the multimodal representations
                    evolve
                    to become more aligned with the DINOv2 embeddings.
                </p>

                <figure style="width: 100%; max-width: 1200px; margin: 0 auto;">
                    <img src="figures/linear_adapter.gif" alt="PCA Alignment Visualization"
                        style="width: 100%; height: auto;">
                    <img src="figures/mlp_adapter.gif" alt="PCA Alignment Visualization"
                        style="width: 100%; height: auto;">
                    <figcaption>
                        <strong>Figure 4.</strong> PCA plots of the embeddings from the DINOv2-small encoder (blue) and
                        our aligned multimodal
                        encoder (yellow) at the first epoch (left) and last epoch (right) of training, for both
                        types of adapters: (A) linear and (B) MLP adapters. The multimodal representations become
                        progressively more aligned with the DINOv2 embeddings
                        during training.
                    </figcaption>
                </figure>

                <h3>Key Hypotheses Supported</h3>

                <p>
                    Our results show that both the weak and strong hypotheses are supported. This indicates that the
                    multimodal representations, which incorporate both image and text information, are more closely
                    aligned with the DINOv2 embeddings than either the image or text representations alone.
                </p>

                <figure style="width: 100%; max-width: 1200px; margin: 0 auto;">
                    <img src="figures/key_hypothesis_results.png" alt="Key Hypothesis Results"
                        style="width: 100%; height: auto;">
                    <figcaption>
                        <strong>Figure 5.</strong> The mutual-KNN kernel alignment metric for our aligned multimodal
                        encoder surpasses both the (left) average of and the better of the two unimodal encoder,
                        supporting both the weak
                        and strong hypotheses, for both (A) linear adapters and (B) MLP adapters. This means that the
                        mutimodal Representations
                        learned by aligning image and text representations are more similar to the DINOv2 embeddings
                        than either modality alone.
                    </figcaption>
                </figure>

                <p>
                    This result is particularly compelling given that DINOv2 is a self-supervised vision model
                    trained
                    <em>without any text supervision</em>. The fact that multimodal alignment with text leads to
                    representations
                    that are more similar to DINOv2, we believe, supports the <strong>Platonic Representation
                        Hypothesis</strong> —
                    suggesting a shared statistical model of reality across modalities.
                </p>

                <h3>Downstream Classification Performance</h3>

                <p>
                    To evaluate the quality of our aligned representations in a downstream task setting, we conducted experiments on the 
                    CIFAR-10 image classification dataset. Our evaluation pipeline involved training linear classifiers on frozen features 
                    extracted from three models: DINOv2-small (our performant baseline), the unimodal ResNet-18 image encoder, and our 
                    aligned multimodal encoder.
                </p>

                <p>
                    The classifiers were intentionally kept simple to evaluate the quality of the learned representations rather than 
                    the classification architecture. Each classifier consisted of a two-layer MLP with architecture:
                </p>

                <p>
                    \[
                    \text{Linear}(d_e \rightarrow 512) \rightarrow \text{ReLU} \rightarrow \text{Dropout}(0.1) \rightarrow 
                    \text{Linear}(512 \rightarrow 10)
                    \]
                </p>

                <p>
                    where \( d_e = 768 \) is the embedding dimension. The classifiers were trained for one epoch using the AdamW optimizer 
                    with a learning rate of \( 1 \times 10^{-3} \).
                </p>

                <h4>Results on CIFAR-10</h4>

                <p>
                    Our experiments with the linear adapter yielded the following results:
                </p>
                <ul>
                    <li>DINOv2-small (baseline): 98.05% accuracy</li>
                    <li>ResNet-18 image encoder: 77.84% accuracy (79.4% relative to DINOv2)</li>
                    <li><strong>Aligned multimodal encoder: 75.69% accuracy (77.2% relative to DINOv2)</strong></li>
                </ul>

                <p>
                    For the MLP adapter:
                </p>
                <ul>
                    <li>DINOv2-small (baseline): 97.95% accuracy</li>
                    <li>ResNet-18 image encoder: 74.91% accuracy (76.5% relative to DINOv2)</li>
                    <li><strong>Aligned multimodal encoder: 72.21% accuracy (73.7% relative to DINOv2)</strong></li>
                </ul>

                <p>
                    The linear adapter demonstrated marginally better performance, with both adapters showing similar patterns in their 
                    relative performance compared to DINOv2. Notably, while the aligned representations perform slightly below the 
                    unimodal image encoder (by approximately 2-3% absolute), they maintain strong relative performance compared to 
                    DINOv2 (>73% relative accuracy for both adapters).
                </p>

                <p>
                    This minor performance gap is expected and can be interpreted positively: our aligned representations successfully 
                    maintain most of the task-relevant visual information while incorporating additional modalities, achieving 77.2% 
                    and 73.7% of DINOv2's performance for linear and MLP adapters respectively. This suggests that the alignment 
                    process preserves core visual features while potentially enriching the representation space with complementary 
                    information from the text modality.
                </p>

                <p>
                    These results align with our kernel analysis findings, demonstrating that our aligned representations effectively 
                    capture meaningful visual features despite not being explicitly optimized for image classification. The fact that 
                    we can achieve >73% relative performance compared to a state-of-the-art vision model while incorporating text 
                    modality suggests that our alignment approach successfully balances modality-specific and shared information in 
                    the representation space.
                </p>

                <details>
                    <summary><em>Expand for schematic of our downstream classification experiment.</em></summary>
                    <img src="figures/downstream_diagram.svg" alt="Downstream Classification Performance">
                </details>
        </section>

        <section id="discussion">
            <h2>Discussion</h2>

            <p>
                In this work, we investigated whether representations from pre-trained unimodal encoders can be aligned
                into a shared multimodal latent space using lightweight adapters. Our experiments provide evidence
                supporting the <strong>Platonic Representation Hypothesis (PRH)</strong> proposed by Huh et al. (2024).
                Specifically, we observed that aligning image and text embeddings via contrastive learning leads to
                multimodal representations that are more closely aligned with the DINOv2-small features compared to
                unimodal representations alone.
                Our alignment approach was inspired by the <strong>CLIP framework</strong> (Radford et al., 2021), which
                demonstrated the power of aligning cross-modal representations using paired image-text data. Unlike
                CLIP, which requires joint training of encoders, our method achieves alignment post hoc with frozen
                encoders and lightweight adapters, providing a scalable alternative.
            </p>

            <h3>Key Insights</h3>

            <p>
                Our experiments revealed three key insights:
            </p>

            <ul>
                <li>
                    <strong>Surprising Cross-Modal Alignment:</strong>
                    Despite <strong>DINOv2-small</strong> being a self-supervised vision model with no text supervision,
                    aligning image and text representations improved their similarity to DINOv2 embeddings. This
                    suggests an underlying universality in representation spaces, supporting the PRH's notion of
                    convergence toward a shared statistical model of reality. The result is notable as it implies that
                    contrastive alignment between modalities — even when one modality was not part of the performant
                    model's training — can enhance representational alignment.
                </li>

                <li>
                    <strong>Creative Reformulation of Contrastive Views:</strong>
                    Our use of the dual-encoder contrastive objective generalizes the traditional notion of “views” from
                    single-modality augmentations to different modalities representing distinct views of a hypothesized
                    common underlying reality. This reformulation, inspired by the Platonic Representation Hypothesis,
                    is a unique perspective that distinguishes our framework from conventional contrastive learning
                    approaches.
                </li>

                <li>
                    <strong>Empirical Support for Key Hypotheses:</strong>
                    Our results validated both the <strong>weak</strong> and <strong>strong hypotheses</strong>
                    formulated in the mathematical framework. The aligned multimodal kernel \( K_{\text{align}} \)
                    consistently surpassed the average and the best of the unimodal kernels (\( K_X \) and \( K_Y \)),
                    highlighting that the combined information from multiple modalities can lead to representations that
                    better approximate those of performant models.
                </li>
            </ul>

            <h3>Non-Trivial Alignment to DINOv2</h3>

            <p>
                Notably, our multimodal encoder was not explicitly optimized to produce embeddings that match those from
                the DINOv2-small feature extractor. The dual-encoder contrastive loss we used does not include DINOv2 in
                any way. This makes it compelling that the representations produced by our aligned encoder \(
                h_{\text{multi}}(x, y) \) are <em>more similar</em> to DINOv2-small embeddings than the unimodal image
                encoder alone. This refutes the possibility that the multimodal encoder is simply regurgitating the
                image encoder's output and ignoring the text modality. The results demonstrate the true benefit of
                multimodal alignment.
            </p>


            <h3>Potential Limitations</h3>

            <ul>
                <li>
                    <strong>Dataset Bias:</strong>
                    The <strong>Flickr30k dataset</strong> primarily contains images and captions depicting people in
                    everyday scenarios. This limited diversity may constrain the generalizability of our findings.
                    Exploring alignment on more varied datasets could yield further insights.
                </li>

                <li>
                    <strong>Restricted Modalities:</strong>
                    Our study focused on aligning image and text modalities. Extending this framework to incorporate
                    audio or video data would provide a more comprehensive test of the alignment mechanism and PRH.
                </li>
            </ul>

            <h3>Conclusion</h3>

            <p>
                Our work demonstrates that multimodal alignment via contrastive learning is a promising
                approach for achieving efficient, scalable representation learning. The observed convergence toward
                DINOv2 embeddings supports the notion of a <strong>universal Platonic representation</strong>, where
                aligning multiple modalities reveals shared underlying structures in representation spaces. This result
                builds upon insights from both the <strong>CLIP framework</strong> and the <strong>Platonic
                    Representation Hypothesis</strong>, underscoring the potential of multimodal alignment in advancing
                our understanding of representation learning.
            </p>
        </section>



        <section id="future-directions">
            <h2>Future Directions</h2>

            <h3>1. Leveraging Multiple Captions for Alignment</h3>
            <p>
                In our current work, we use only the first caption from the Flickr30k dataset as a positive anchor for
                each image. To improve alignment quality, we will utilize all five captions per image. For each data
                point \( i \), we redefine the input as a sixtuplet \( (x^{(i)}, y_1^{(i)}, y_2^{(i)}, \dots, y_5^{(i)})
                \), where \( y_k^{(i)} \) represents the \( k \)-th caption.
            </p>
            <p>
                The contrastive loss then becomes:
            </p>
            <p>
                \[
                \mathcal{L}_{\text{multi-caption}} = -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp\left(\frac{1}{5\tau}
                \sum_{k=1}^5 f_\text{image}(x^{(i)}) \cdot g_\text{text}(y_k^{(i)})\right)}{\sum_{j=1}^N
                \exp\left(\frac{1}{5\tau} \sum_{k=1}^5 f_\text{image}(x^{(i)}) \cdot g_\text{text}(y_k^{(j)})\right)}.
                \]
            </p>
            <p>
                This approach leverages the diversity of captions to potentially create more robust and semantically
                aligned representations.
            </p>

            <h3>2. Scaling to Richer Multimodal Datasets</h3>
            <p>
                We plan to extend our experiments beyond images and text by integrating a third modality, such as audio.
                For example, using the <strong><a
                        href="https://www.robots.ox.ac.uk/~vgg/data/vggsound/">VGGSound</strong></a> dataset and a
                pre-trained audio encoder like
                <strong><a href="https://huggingface.co/docs/transformers/en/model_doc/wav2vec2">Wav2Vec</strong></a>,
                we can evaluate whether incorporating audio further improves kernel alignment
                metrics compared to the two-modality setting.
            </p>
            <p>
                After that, we consider expanding our approach to as many modalities as there exists a suitable dataset
                for. To achieve this, we will generalize the dual-encoder contrastive loss to a <strong>multi-encoder
                    contrastive loss</strong> for \( M \) modalities. Specifically, the loss can be expressed as:
            </p>
            <p>
                \[
                \mathcal{L}_{\text{multi-encoder}} = -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp\left( \sum_k
                \prod_{m=1}^M
                ( \mathbf{f_m^{(i)}} )_k /
                \tau\right)}{\sum_{j=1}^N \exp\left(\sum_k \prod_{m=1}^M ( \mathbf{f_m^{(j)}} )_k / \tau\right)},
                \]
            </p>
            <p>
                where \( ( \mathbf{f_m^{(i)}} )_k \) represents the \( k \)-th entry in the embeddings of the \( i
                \)-th
                data point produced by the \( m \)-th modality encoder, and \( \prod_{m=1}^M \) is an element-wise
                product.
            </p>
            <p>
                A potential challenge is that VGGSound provides video IDs and timestamps rather than direct downloads,
                complicating dataset acquisition.
            </p>

            <h3>3. Zero-Shot Evaluation with Pre-Trained Heads</h3>
            <p>
                We plan to evaluate the aligned
                representations directly using pre-trained classifier heads (e.g., those available in the <a
                    href="https://github.com/facebookresearch/dinov2?tab=readme-ov-file">DINOv2 GitHub repository</a>).
                Specifically, we will replace DINOv2-small's feature extractor with our aligned multimodal encoder \(
                h_\text{multi} \) and measure classification performance on tasks like ImageNet. This approach will
                assess the quality of the aligned embeddings in a purely <strong>zero-shot</strong> setting.
            </p>


        </section>


    </main>

    <footer>
        <p>Quilee Simeon & Gabe Manso | MIT Deep Learning Final Project Blog | December 2024</p>
    </footer>

    <style>
        /* General Styling */
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            margin: 0;
            padding: 0;
            background-color: #f9f9f9;
        }

        /* Header Styling */
        header {
            background: #005f73;
            color: white;
            padding: 20px 0;
            text-align: center;
        }

        header h1 {
            margin: 0;
            font-size: 2.5em;
        }

        /* Section Styling */
        main {
            padding: 20px;
            max-width: 800px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
        }

        section {
            margin-bottom: 20px;
        }

        section h2 {
            border-bottom: 2px solid #005f73;
            padding-bottom: 5px;
            color: #005f73;
            font-size: 1.5em;
        }

        /* List Styling */
        ul {
            padding-left: 20px;
            list-style-type: disc;
        }

        ul li {
            margin-bottom: 10px;
        }

        /* Footer Styling */
        footer {
            text-align: center;
            padding: 10px 0;
            background: #005f73;
            color: white;
            font-size: 0.9em;
        }

        /* Code and Pre-formatted Text */
        code,
        pre {
            background: #e0f4f4;
            padding: 2px 5px;
            border-radius: 3px;
            font-family: "Courier New", Courier, monospace;
            font-size: 0.95em;
        }

        /* Image and Visualization Placeholder Styling */
        img {
            max-width: 100%;
            height: auto;
            margin: 10px 0;
        }

        p em {
            font-style: italic;
            color: #666;
        }

        figure {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 10px;
        }

        .figures-container {
            display: flex;
            justify-content: center;
            gap: 20px;
        }

        figure img {
            width: 400px;
            height: auto;
        }

        figcaption {
            text-align: center;
            font-size: 14px;
            color: #555;
        }
    </style>
</body>

</html>