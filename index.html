<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>
    <title>Aligning Modalities: Efficient Multimodal Representation Learning</title>
    <link rel="stylesheet" href="styles/style.css">
</head>

<body>
    <header>
        <h1>Towards the Platonic Representation via Multimodal Contrastive Alignment</h1>
    </header>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>
            <p>
                Deep learning models are typically trained to transform raw data into representations optimized for
                specific tasks. Recently, two lines of research have inspired a deeper inquiry into the nature of these
                representations. The CLIP framework demonstrated the utility of aligning representations across
                modalities, using paired image-text data to train joint embeddings for cross-modal retrieval. Meanwhile,
                the Platonic Representation Hypothesis posits that performant models converge toward a shared
                statistical model of reality in their representation spaces, suggesting a potential universality
                underlying learned representations.
            </p>
            <p>
                This project bridges these ideas by exploring whether representations from disparate pre-trained
                unimodal neural networks can be aligned into a shared multimodal latent space, inspired by the joint
                embedding approach of CLIP and motivated by the convergence hypothesis of Platonic Representations. The
                proposed framework uses frozen unimodal encoders (e.g., ResNet-18 for images, GPT-2 for text) with
                learned linear adapters to align representations across modalities. Our aim is to determine if such
                aligned representations better approximate those of larger, more performant models (e.g., DINOv2-small).
            </p>
            <p>
                Inspired by the success of CLIP in aligning representations across modalities and the theoretical
                insights of the Platonic Representation Hypothesis, we propose a framework that aligns pre-trained
                unimodal encoders into a shared multimodal latent space using simple linear adapters. By leveraging
                frozen encoders such as ResNet-18 for images and GPT-2 for text, we aim to achieve alignment without
                retraining these large models, enabling scalability to new modalities with minimal overhead.
            </p>
            <figure>
                <img src="figures/conceptual_contrastive.svg" alt="Conceptual Multimodal Alignment">
                <figcaption>
                    <strong>Figure.</strong> Conceptual illustration of our multimodal alignment framework. Image and
                    text representations
                    from pre-trained encoders are aligned into a shared latent space using learned adapters. Matching
                    image-text pairs from the Flickr30k dataset are contrastively
                    aligned in
                    the latent space, while non-matching pairs are pushed apart. The aim is to approximate a proxy of
                    the Platonic
                    representations.
                </figcaption>
            </figure>


            <h3>Motivation</h3>
            <p>
                Our approach is motivated by three key insights:
            </p>
            <ol>
                <li><strong>Inspiration from CLIP:</strong> The CLIP framework demonstrated that cross-modal
                    representations could be aligned through paired data and contrastive learning. However, it requires
                    joint training of encoders, limiting extensibility to additional modalities. Our method decouples
                    the encoders, focusing instead on aligning their outputs via lightweight adapters.</li>
                <li><strong>Testing the Platonic Representation Hypothesis:</strong> This hypothesis posits that
                    performant models converge toward a shared statistical model of reality in their representation
                    spaces. By aligning diverse unimodal encoders, we provide a testbed for exploring whether this
                    convergence can be achieved explicitly.</li>
                <li><strong>Scalability and Modularity:</strong> Traditional multimodal models often require joint
                    training on extensive datasets. By aligning frozen encoders post hoc, our framework supports modular
                    integration of new modalities, enabling efficient experimentation.</li>
            </ol>
        </section>

        <section id="related-work">
            <h2>Related Work</h2>
            <p>
                Our project builds on several important advances in multimodal representation learning and theoretical
                insights into model convergence:
            </p>
            <ol>
                <li>
                    <strong>Platonic Representation Hypothesis (PRH):</strong> Huh et al. (2024) proposed that
                    performant
                    models converge toward a shared statistical model of reality in their representation spaces,
                    regardless of their training modality. This hypothesis underpins our framework, which aims to
                    explicitly align unimodal encoders to test this convergence hypothesis.
                </li>
                <li>
                    <strong>CLIP:</strong> Radford et al. (2021) introduced CLIP, a model that learns joint multimodal
                    representations using contrastive learning on paired image-text datasets. CLIP's success
                    demonstrates the power of cross-modal embeddings but requires joint training of encoders, which our
                    framework aims to circumvent.
                </li>
                <li>
                    <strong>Linear Mapping from Image to Text Space:</strong> Merullo et al. (2022) showed that simple
                    linear transformations can align visual representations to text spaces, enabling cross-modal tasks
                    like visual question answering. Their findings inspired our use of linear adapters for modality
                    alignment.
                </li>
                <li>
                    <strong>Grounding Language Models to Images:</strong> Koh et al. (2023) extended the idea of
                    multimodal alignment by mapping language models to visual inputs and outputs. This work highlights
                    the potential of lightweight transformations for cross-modal integration.
                </li>
                <li>
                    <strong>DINOv2:</strong> Oquab et al. (2023) introduced DINOv2, a self-supervised vision transformer
                    model that
                    generates robust embeddings. As our performant reference model, DINOv2-small provides a benchmark
                    for
                    evaluating the quality of our aligned multimodal representations.
                </li>
                <!-- TODO: Add this paper to the related work section: 
                Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, pages 9929–9939. PMLR, 2020. -->
            </ol>
        </section>

        <section id="hypothesis">
            <h2>Hypothesis</h2>
            <p>
                Our work is grounded in the following hypotheses:
            </p>
            <ul>
                <li>A shared latent space exists where unimodal representations from different encoders can be aligned
                    through linear transformations.</li>
                <li>Aligning these representations produces embeddings that closely approximate those of performant
                    models, such as DINOv2-small, as measured by kernel alignment metrics.</li>
                <li>Multimodal alignment captures mechanisms of representation convergence, offering empirical evidence
                    for the Platonic Representation Hypothesis.</li>
            </ul>
        </section>

        <section id="mathematical-framework">
            <h2>Mathematical Framework</h2>
            <p>
                At the core of our project is the hypothesis that pre-trained unimodal representations can be aligned
                into a shared multimodal latent space. This section formalizes the mathematical structure underlying our
                framework, detailing how representations are extracted, aligned, and evaluated.
            </p>

            <h3>Multimodal Data Representation</h3>
            <p>
                Let the world generate raw multimodal data:
            </p>
            <p>
                \[
                \mathcal{D}_\text{world} = \left\{\left(\theta^{(i)}, \psi^{(i)}, \phi^{(i)}, \dots \right)
                \right\}_{i=1}^N, \quad \theta \in \mathcal{\Theta}, \psi \in \mathcal{\Psi}, \phi \in \mathcal{\Phi},
                \dots
                \]
            </p>
            <p>
                where \( \mathcal{\Theta}, \mathcal{\Psi}, \mathcal{\Phi} \) represent different modalities (e.g.,
                image, text, audio), and \( \theta, \psi, \phi \) are specific instances of these modalities.
            </p>
            <p>
                For simplicity, we focus on a two-modality setting with images (\( \mathcal{X} \)) and text (\(
                \mathcal{Y} \)):
            </p>
            <p>
                \[
                \mathcal{D}_\text{world} = \left\{\left(x^{(i)}, y^{(i)}\right)\right\}_{i=1}^N, \quad x \in
                \mathcal{X}, y \in \mathcal{Y}.
                \]
            </p>
            <figure>
                <img src="figures/platonic_representation.svg" alt="Platonic Representation Hypothesis">
                <figcaption>
                    <strong>Figure.</strong> The Platonic Representation Hypothesis suggests that
                    representations from different modalities converge to a shared Platonic representation. We
                    hypothesize that contrastive learning can actively drive this convergence by aligning
                    representations from different modalities into a shared latent space.
                </figcaption>
            </figure>

            <h3>Learned Adapters</h3>
            <p>
                We use frozen pre-trained encoders to extract representations from each modality:
            </p>
            <p>
                \[
                X_\text{enc}: \mathcal{X} \rightarrow \mathbb{R}^{d_x}, \quad Y_\text{enc}: \mathcal{Y} \rightarrow
                \mathbb{R}^{d_y},
                \]
            </p>
            <p>
                where \( d_x \) and \( d_y \) are the embedding dimensions for images and text, respectively.
            </p>
            <p>
                To project these representations into a shared latent space \( \mathbb{R}^{d_e} \), we introduce learned
                adapters:
            </p>
            <p>
                \[
                W_x : \mathbb{R}^{d_x} \rightarrow \mathbb{R}^{d_e}, \quad W_y : \mathbb{R}^{d_y} \rightarrow
                \mathbb{R}^{d_e}
                \]
            </p>
            <p>
                We explore both <strong>linear adapters</strong> and <strong>2-layer MLP adapters</strong>. The adapted
                encoders:
            </p>
            <p>
                \[
                \begin{aligned}
                f_\text{image}: \mathcal{X} \rightarrow \mathbb{R}^{d_e}, \quad f_\text{image} &= W_x \circ X_\text{enc}
                \\
                g_\text{text}: \mathcal{Y} \rightarrow \mathbb{R}^{d_e}, \quad g_\text{text} &= W_y \circ Y_\text{enc}
                \end{aligned}
                \]
            </p>
            <p>
                \( f_\text{image} \) and \( g_\text{text} \) map images and text, respectively, to the shared latent
                space \( \mathbb{R}^{d_e} \).
            </p>

            <h3>Dual-Encoder Contrastive Objective</h3>
            <p>
                To align the representations, we use a <strong>dual-encoder contrastive loss</strong>, inspired by the
                formulation in the <a href="https://phillipi.github.io/prh/">PRH</a> and Homework 4:
            </p>
            <figure>
                <img src="figures/homework4_contrastive.jpg" alt="Dual-Encoder Contrastive Loss">
            </figure>

            <p>
                The loss encourages
                representations from matching pairs to be similar while pushing apart representations from non-matching
                pairs:
            </p>
            <p>
                \[
                \mathcal{L}_{\text{contrastive}} = -\frac{1}{N} \sum_{i=1}^N \log
                \frac{\exp\left(f_\text{image}(x^{(i)})
                \cdot g_\text{text}(y^{(i)}) / \tau\right)}{\sum_{j=1}^N \exp\left(f_\text{image}(x^{(i)}) \cdot
                g_\text{text}(y^{(j)}) / \tau\right)},
                \]
            </p>
            <p>
                where \( \tau \) is the temperature parameter that scales the similarity scores.
            </p>


            <h3>Interpretation and Connection to PRH</h3>
            <p>
                The <strong>Platonic Representation Hypothesis (PRH)</strong> posits that representations from different
                modalities converge toward a shared Platonic representation. In our framework, we hypothesize that
                <strong>contrastive alignment</strong> between the unimodal image \( f_{\text{image}} \) and text \(
                g_{\text{text}} \) encoders can drive this convergence toward the Platonic representation.
            </p>

            <p>
                In the ideal case of perfect contrastive alignment (achieved by minimizing the diameter and maximizing
                the margin), the outputs of \( f_{\text{image}}(x) \) and \( g_{\text{text}}(y) \) for positive
                pairs would map to the <strong>same embedding vector</strong> in the shared latent space i.e. \(
                f_{\text{image}}(x^{(i)})
                = g_{\text{text}}(y^{(i)}) \)
            </p>

            <p>
                Our thesis is that this limiting vector represents the Platonic representation. However, in practice,
                due to finite data, model capacity, and noise, the outputs of \( f_{\text{image}} \) and \(
                g_{\text{text}} \) are typically <strong>close</strong> (as measured by the \( L_2
                \)-distance on the unit hypersphere \( \mathbb{S}^{d_e-1} \)) but not identical i.e. \(
                f_{\text{image}}(x^{(i)})
                \approx g_{\text{text}}(y^{(i)}) \).
            </p>

            <p>
                Given this, we define our <strong>aligned multimodal encoder</strong> \( h_\text{multi} \) as a function
                that averages the embeddings ooutput by the adapted image and text encoders, i.e.:
            </p>
            \[
            h_\text{multi}: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}^{d_e}, \quad h_\text{multi}(x, y) =
            \frac{1}{2}
            \left(f_{\text{image}}(x) + g_{\text{text}}(y)\right).
            \]
            </p>

            <p>
                We call the output \( h_\text{multi}(x^{(i)}, y^{(i)}) \) the <strong>aligned multimodal
                    representation</strong>
                and claim that this is an <strong>approximation</strong> of the Platonic representation of the \(i\)-th
                data point. While the true
                Platonic representation is unknown, we follow the PRH
                conjecture that performant models, such as the DINOv2 models by <a
                    href="https://arxiv.org/abs/2304.07193">Oquab et al. (2023)</a>, produce representations that are
                converging toward
                it. We will use the features extracted by a pretrained <a
                    href="https://huggingface.co/facebook/dinov2-small">DINOv2-small</a> backbone as a
                <strong>proxy</strong> for the Platonic representation.
            </p>

            <p>
                We evaluate the quality of our aligned multimodal encoder \( h_\text{multi} \) its representations to
                those of the DINOv2-small feature extractor using a <strong>kernel alignment metric</strong>.
            </p>

            <h3>Kernel Alignment Metric</h3>
            <p>
                We characterize representations in terms of their <strong>kernels</strong>, which capture the similarity
                between data points. Two representations are considered <strong>aligned</strong> if their kernels are
                similar for corresponding inputs. For example, if the text encoder \( g_{\text{text}} \) is aligned with
                the image encoder \( f_{\text{image}} \), the similarity between text representations of "apple" and
                "orange" should correspond closely to the similarity between their image representations:
            </p>

            <figure>
                <img src="figures/similarity_metric.jpg" alt="Similarity Metric">
            </figure>

            <p>
                To quantify this alignment, we use the <strong>mutual \(k\)-nearest neighbor (mutual-KNN)</strong>
                kernel alignment metric \( m \), as introduced by <a href="https://arxiv.org/abs/2405.07987">Huh et al.
                    (2024)</a>. We define the kernels as:
            </p>

            <ul>
                <li>
                    <strong>Unimodal Kernels:</strong>
                    <p>
                        \[
                        K_X(i, j) = \langle f_{\text{image}}(x^{(i)}), f_{\text{image}}(x^{(j)}) \rangle, \quad K_Y(i,
                        j) =
                        \langle g_{\text{text}}(y^{(i)}), g_{\text{text}}(y^{(j)}) \rangle
                        \]
                    </p>
                </li>
                <li>
                    <strong>Aligned Multimodal Kernel:</strong>
                    <p>
                        \[
                        K_{\text{align}}(i, j) = \langle h_\text{multi}(x^{(i)}, y^{(i)}) , h_\text{multi}(x^{(j)},
                        y^{(j)}) \rangle,
                        \]
                    </p>

                </li>
                <li>
                    <strong>Performant Model Kernel:</strong>
                    <p>
                        \[
                        K_\text{DINOv2}(i, j) = \langle \text{DINOv2}(x^{(i)}), \text{DINOv2}(x^{(j)}) \rangle
                        \]
                    </p>
                    <p>
                        We emphasize that DINOv2-small is an <strong>image-only</strong> model so its kernel is computed
                        using samples from the image modality as input.
                    </p>
                </li>
            </ul>

            <h4>Before Training</h4>
            <p>
                We compute the alignment metrics for the unimodal kernels relative to the performant model kernel:
            </p>
            <p>
                \[
                m(K_X, K_{\text{DINOv2}}), \quad m(K_Y, K_{\text{DINOv2}}).
                \]
            </p>

            <h4>After Training</h4>
            <p>
                We evaluate the mutual-KNN alignment for the aligned multimodal kernel against the performant model
                kernel:
            </p>
            <p>
                \[
                m(K_{\text{align}}, K_{\text{DINOv2}}).
                \]
            </p>

            <h4>Key Hypothesis</h4>
            <p>
                We hypothesize that the aligned multimodal kernel achieves higher similarity with \( K_{\text{DINOv2}}
                \) than the average of the unimodal kernels:
            </p>
            <p>
                \[
                m(K_{\text{align}}, K_{\text{DINOv2}}) > \text{avg}\left(m(K_X, K_{\text{DINOv2}}), m(K_Y,
                K_{\text{DINOv2}})\right).
                \]
            </p>

            <p>
                A stronger version of this hypothesis posits that the aligned multimodal kernel surpasses even the best
                unimodal kernel:
            </p>
            <p>
                \[
                m(K_{\text{align}}, K_{\text{DINOv2}}) > \max\left(m(K_X, K_{\text{DINOv2}}), m(K_Y,
                K_{\text{DINOv2}})\right).
                \]
            </p>

            <p>
                This would suggest an interesting emergent-like property where the whole (multimodal representation) is
                greater than either its parts (unimodal representations) alone,
            </p>

        </section>

        <section id="methodology">
            <h2>Methodology</h2>

            <p>
                All code we used for this project is implemented in this <a
                    href="https://colab.research.google.com/drive/1BkyPko0x-8CL41Z-VSmyxI41B90hB2Fc?usp=sharing">Google
                    Colab notebook</a>.
            </p>

            <h3>Data Pipeline</h3>
            <p>
                We construct our world dataset \( \mathcal{D}_\text{world} \) from first 12,800 samples from
                <strong>Flickr30k</strong>
                dataset, which contains paired image-caption samples. Each image \( x^{(i)} \) in Flickr30k dataset is
                associated with
                5 captions, but for our dataset we used only the first caption \( y^{(i)} \) as a single postive
                anchor The data
                pipeline processes these pairs to generate tensors compatible with our models. Key stages of the
                pipeline include:
            </p>
            <ul>
                <li><strong>Image Processing:</strong>
                    <ul>
                        <li>Resize images to \(224 \times 224\) pixels.</li>
                        <li>Normalize using ImageNet statistics.</li>
                        <li>Convert to tensors suitable for ResNet-18.</li>
                    </ul>
                </li>
                <li><strong>Text Processing:</strong>
                    <ul>

                        <li>Tokenize captions using the DistilBERT tokenizer.</li>
                        <li>Pad or truncate tokens to a maximum length of 64.</li>
                        <li>Convert tokens to tensors suitable for DistilBERT.</li>
                    </ul>
                </li>
                <li><strong>Data Loading:</strong>
                    <ul>
                        <li>Batch size: 256</li>
                        <li>75%-25% train-test split.</li>
                        <li>Optimized for GPU utilization with shuffling and memory pinning.</li>
                    </ul>
                </li>
            </ul>
            <figure>
                <img src="figures/data_pipeline.jpg" alt="Data Pipeline">
                <figcaption><strong>Figure.</strong> Schematic/flow chart diagram illustraing our data pipeline which
                    preprocesses images and text for input into the
                    model.</figcaption>
            </figure>

            <h3>Model Architecture</h3>
            <p>
                The architecture consists of frozen pre-trained encoders and trainable adapters that align
                representations in a shared multimodal space.
            </p>
            <ul>
                <li><strong>Frozen Image Encoder:</strong> ResNet-18 (input type: RGB image; output dimension: 512).
                </li>
                <li><strong>Frozen Text Encoder:</strong> DistilBERT (input type: text string; output dimension: 768).
                </li>
                <li><strong>Performant Model Backbone:</strong> DINOv2-small (input type: RGB image; output
                    dimension: 768).</li>
                <li><strong>Adapters:</strong>
                    <ul>
                        <li><strong>Linear Adapter:</strong> Projects frozen encoder outputs to a shared dimension of
                            768,
                            which is embedding dimension of DINOv2-small's features:
                            \[
                            W_x : \mathbb{R}^{512} \to \mathbb{R}^{768}, \quad W_y : \mathbb{R}^{768} \to
                            \mathbb{R}^{768}
                            \]
                        </li>
                        <li><strong>MLP Adapter:</strong> A two-layer MLP with GELU activation and LayerNorm:
                            \[
                            \operatorname{MLP}(z) = \operatorname{LayerNorm}(\operatorname{GELU}(W_2
                            \operatorname{GELU}(W_1 z))), \quad z \in \mathcal{X} \text{ or } z \in \mathcal{Y}
                            \]
                        </li>
                    </ul>
                </li>
            </ul>
            <p>
                After training, the adapted image encoder, text encoder, and the aligned multimodal encoder can be used
                to extract features for downstream tasks, such as image classification.
            </p>

            <figure>
                <img src="figures/img_txt_concept.svg" alt="Multimodal Alignment Architecture">
                <figcaption><strong>Figure.</strong> Conceptual illustration of our approach to using frozen
                    pretrained encoders to map single modalities (image and text) into shared latent space.
                    We encode image and text pairs using frozen pretrained ResNet-18 and DistilBERT encoders. We use
                    learnable adapters to map from the dimensions of the embeddings of these frozen encoders to a the
                    dimension of the shared multimodal latent space which is set to be equal to the emdedding dimension
                    of a performant model.
                </figcaption>
            </figure>

            <h3>Training Procedure</h3>
            <p>
                We train the adapters for <strong>15 epochs</strong> using a dual-encoder contrastive loss with
                temperature
                scaling (\(\tau = 0.07\)).
            </p>

            <p>
                \[
                \mathcal{L} = -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp\left(f_\text{image}(x^{(i)}) \cdot
                g_\text{text}(y^{(i)}) / \tau\right)}{\sum_{j=1}^N \exp\left(f_\text{image}(x^{(i)}) \cdot
                g_\text{text}(y^{(j)}) / \tau\right)}.
                \]
            </p>
            <p>The training loop consists of:</p>
            <ol>
                <li>Forward pass through frozen encoders and adapters.</li>
                <li>Compute contrastive loss for matching and non-matching pairs.</li>
                <li>Update adapter weights using the AdamW optimizer with:
                    <ul>
                        <li>Learning rate: \(5 \times 10^{-5}\) (cosine decay with warmup).</li>
                        <li>Gradient clipping (norm: 1.0).</li>
                        <li>Weight decay: 0.01.</li>
                    </ul>
                </li>
            </ol>
            <p>
                We use PCA to visualize the alignment process between the DINOv2-small embeddings and the aligned
                multimodal
                embeddings. First, we apply the DINOv2-small feature extractor to our test set and compute the principal
                components (PCs) of the resulting \(3200 \times 784\) matrix of embeddings. During training, at each
                epoch,
                we project the aligned multimodal embeddings onto the same PCs and plot the first two principal
                component
                scores for both sets of embeddings. This 2D visualization shows how the multimodal representations
                evolve
                to become more aligned with the DINOv2 embeddings.
            </p>


            <figure>
                <img src="figures/training_pipeline.jpg" alt="Training Procedure Diagram">
                <figcaption><strong>Figure.</strong> Schematic/flow chart diagram of our training piepline starting from
                    the processing of the inputs (images and text captions) to the computation of the contrastive loss.
                </figcaption>
            </figure>


            <h3>Evaluation Metrics</h3>
            <p>
                We evaluate alignment using the <strong>mutual-KNN kernel alignment metric</strong> \(m\):
            </p>
            <ul>
                <ul>
                    <li><strong>Unimodal Kernel Alignment:</strong> \( m(K_X, K_{\text{DINOv2}}) \) and \( m(K_Y,
                        K_{\text{DINOv2}}) \).</li>
                    <li><strong>Multimodal Kernel Alignment:</strong> \( m(K_{\text{align}}, K_{\text{DINOv2}}) \).</li>
                    <li><strong>Test Key Hypothesis:</strong> We test our "Key Hypotheses" state in the mathemtical
                        framework (see figure "Hypothesis Test Diagram").</li>
                    <li><strong>Downstream Task:</strong> We use the embeddings from the adapted encoders and the
                        aligned
                        multimodal encoder as feature inputs for a CIFAR-10 classification task. We compare the
                        classification
                        accuracy of:
                        <ul>
                            <li>DinoV2-small embeddings (baseline).</li>
                            <li>Aligned multimodal embeddings.</li>
                            <li>Image-only embeddings.</li>
                        </ul>
                    </li>

                </ul>
            </ul>

            <figure>
                <img src="figures/hypothesis_test.jpg" alt="Hypothesis Test Diagram">
                <figcaption><strong>Figure.</strong> We test our key hypothesis by comparing the kernel alignment
                    metric of the aligned multimodal kernel to the unimodal kernels and the DinoV2 kernel.
                </figcaption>
            </figure>

            <figure>
                <img src="figures/downstream_task.jpg" alt="Downstream Evaluation Diagram">
                <figcaption><strong>Figure.</strong> We perform an experiment where we use the aligned embeddings as
                    feature inputs for a CIFAR-10 classification task. We compare the performance of the aligned
                    embeddings to the image-only embeddings and the DinoV2-small embeddings.
                </figcaption>
            </figure>
        </section>

        <section id="results">
            <h2>Results</h2>

            <h3>Experiment 1: Contrastive Alignment Training</h3>
            <p>
                In this experiment, we train our adapters to align image and text representations using the dual-encoder
                contrastive loss outlined in the <a href="#mathematical-framework">Mathematical Framework</a>.
                We evaluate the alignment performance for two types of adapters:
                <strong>linear adapters</strong> and <strong>2-layer MLP adapters</strong>.
            </p>

            <p>
                We train each type of adapter for <strong>15 epochs</strong> on the <strong>Flickr30k</strong> dataset.
                The alignment performance is measured using the <strong>mutual-KNN kernel alignment metric</strong>.
                Specifically, we compute the following three metrics:
            </p>
            <ul>
                <li>\( m(K_X, K_{\text{DINOv2}}) \): Alignment of the image encoder \( f_\text{image} \)kernel to the
                    DINOv2 kernel.</li>
                <li>\( m(K_Y, K_{\text{DINOv2}}) \): Alignment of the text encoder \( g_\text{text} \) kernel to the
                    DINOv2 kernel.</li>
                <li>\( m(K_{\text{align}}, K_{\text{DINOv2}}) \): Alignment of the multimodal encoder \( h_\text{mult}
                    \) kernel to the DINOv2
                    kernel.</li>
            </ul>

            <h4>Result 1a: Kernel Alignment Improves with Training</h4>
            <p>
                The first key finding of this experiment is that the mutual-KNN kernel alignment metric improves
                significantly after training for both the linear and MLP adapters. This demonstrates that our
                contrastive alignment objective effectively aligns the image and text representations.
            </p>

            <figure>
                <img src="figures/kernel_metric_barplot.svg" alt="Kernel Metric Bar Plot">
                <figcaption>
                    <strong>Figure.</strong> Bar plot showing the mutual-KNN kernel alignment metric before and after
                    training for the linear (left) and MLP (right) adapters. For both adapter types and
                    all encoders, kernel alignment to the DINOv2-small model improves after training with the
                    dual encoder contrastive objective.
                </figcaption>
            </figure>

            <p>
                To further visualize this improvement, we perform PCA on the embeddings produced by the aligned
                multimodal
                encoder and compare them to the DINOv2 embeddings. The PCA plots show how the multimodal representations
                evolve during training to better match the DINOv2 embeddings.
            </p>

            <figure>
                <img src="figures/pca_alignment.svg" alt="PCA Alignment Visualization">
                <figcaption>
                    <strong>Figure.</strong> PCA plots of the DINOv2 embeddings (blue) and aligned multimodal
                    representations (yellow)
                    at the first epoch (left) and last epoch (right) of training. The multimodal representations become
                    progressively more aligned with the DINOv2 embeddings during training.
                </figcaption>
            </figure>

            <h4>Result 1b: Key Hypotheses Supported</h4>
            <p>
                We also test the key hypotheses outlined in the <a href="#mathematical-framework">Mathematical
                    Framework</a>:
            </p>
            <ul>
                <li>
                    <strong>Weak Hypothesis:</strong> The aligned multimodal kernel achieves higher similarity with
                    the DINOv2 kernel than the average of the unimodal kernels:
                    \[
                    m(K_{\text{align}}, K_{\text{DINOv2}}) > \text{avg}(m(K_X, K_{\text{DINOv2}}), m(K_Y,
                    K_{\text{DINOv2}})).
                    \]
                </li>
                <li>
                    <strong>Strong Hypothesis:</strong> The aligned multimodal kernel surpasses even the best unimodal
                    kernel:
                    \[
                    m(K_{\text{align}}, K_{\text{DINOv2}}) > \max(m(K_X, K_{\text{DINOv2}}), m(K_Y, K_{\text{DINOv2}})).
                    \]
                </li>
            </ul>

            <p>
                The results show that both the weak and strong hypotheses are supported. This indicates that the
                multimodal representations, which incorporate both image and text information, are more closely aligned
                with the DINOv2 embeddings than either the image or text representations alone.
            </p>

            <figure>
                <img src="figures/key_hypothesis_results.svg" alt="Key Hypothesis Results">
                <figcaption>
                    <strong>Figure.</strong> The mutual-KNN kernel alignment metric for the aligned multimodal kernel
                    surpasses both the average and the best of the unimodal kernels, supporting both the weak and
                    strong hypotheses for both adapter types.
                </figcaption>
            </figure>

            <p>
                This result is particularly compelling given that DINOv2 is a self-supervised vision model trained
                <em>without any text supervision</em>. The fact that multimodal alignment with text leads to
                representations
                that are more similar to DINOv2, we believe, supports the <strong>Platonic Representation
                    Hypothesis</strong> —
                suggesting a shared statistical model of reality across modalities.
            </p>


            <h3>Experiment 2: Downstream Classification Task</h3>
            <p>
                To assess the quality of the aligned embeddings, we use them for a downstream image classification task
                on
                the CIFAR-10 dataset. The goal is to determine how well the aligned multimodal representations preserve
                task-relevant information compared to the DinoV2-small embeddings and the image-only embeddings.
            </p>

            <p>
                After training the adapters, we freeze the adapted encoders (image \( f_\text{image}\), and
                multimodal ( h_\text{multi}\)) and use them
                to
                extract features for the CIFAR-10 images. A simple classifier head is then trained on these features to
                measure accuracy. We perform this experiment with both linear and MLP adapters and compare the
                following:
            </p>
            <ul>
                <li><strong>DinoV2 Features:</strong> Baseline accuracy using DinoV2-small embeddings.</li>
                <li><strong>Aligned Multimodal Features:</strong> Accuracy using the aligned multimodal embeddings.</li>
                <li><strong>Image-Only Features:</strong> Accuracy using the image encoder's adapted embeddings.</li>
            </ul>

            <table>
                <caption><strong>Table 1.</strong> CIFAR-10 classification accuracy using different types of embeddings
                    for both MLP and Linear adapters.</caption>
                <thead>
                    <tr>
                        <th>Adapter Type</th>
                        <th>DinoV2</th>
                        <th>Aligned Multimodal</th>
                        <th>Image-Only</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>MLP Adapter</strong></td>
                        <td>97.73%</td>
                        <td>73.29%</td>
                        <td>74.91%</td>
                    </tr>
                    <tr>
                        <td><strong>Linear Adapter</strong></td>
                        <td>97.96%</td>
                        <td>76.49%</td>
                        <td>78.21%</td>
                    </tr>
                </tbody>
            </table>

            <p>
                These results show that while the aligned multimodal embeddings perform worse than the DinoV2 baseline,
                they achieve reasonable accuracy compared to the image-only embeddings. This suggests that the alignment
                process captures meaningful multimodal features that are useful for downstream tasks, supporting the
                effectiveness of our approach.
            </p>

        </section>


        <section id="conclusion">
            <h2>Conclusion</h2>
            <p>
                This project illustrates a scalable and efficient approach to aligning unimodal encoders into a shared
                multimodal space. By leveraging pre-trained models and lightweight adapters, we unlock potential
                applications in resource-efficient AI and multimodal representation learning. Our findings highlight the
                promise of this methodology and pave the way for further exploration into the convergence of
                representations across modalities.
            </p>
        </section>

        <section id="discussion">
            <!-- TODO: Add to discussion. Highlight how multimodal alignment enables cross-modality benefits, particularly for modalities like
            text that the performant model DINOv2 wasn't explicitly trained on. Write in discussion about how it is  is a surprising outcome 
            since DinoV2 has no text pretraining. Hence, other than the explanations provided by the platonic representation hypothesis, we don't have any immediately obvious explanation for why training to contrastively align
            image and text representations would result in more similiarity  to DinoV2s representations. So this result is really a strong hint at the reality of a platonic representation. -->
            <!-- TODO: Add to discussion - possible limitation of our choice of dataset is that Flickr30k images and captions
            focus on people involved in everyday activities and events. -->
            <!-- TOOD: Add to discussion: Anout the complexity of adapters impact on alignent. We observed that linear
            adapters slightly outperform MLP adapters in downstream tasks, possibly because simpler transformations
            preserve global structure better. -->
            <!-- TODO: Need to update discussion in light of the results that show that both the weak and strong hypotheses
            are supported. Discuss ptential significance, if any of the strong hypothesis. -->
            <h2>Discussion</h2>
            <p>
                In this project, we build upon a central premise of the Platonic Representation Hypothesis (PRH)
                proposed by Huh et al. (2024): the idea that the representations learned by performant models converge
                toward an idealized, universal representation of the world — a <strong>Platonic representation</strong>.
                We take this hypothesis as a starting point and assume that the representations produced by
                <strong>DINOv2-small</strong> serve as a reasonable approximation of this Platonic representation.
            </p>

            <p>
                Our primary conjecture is that one mechanism to achieve this Platonic representation is through the
                alignment of representations from different modalities into a shared latent space. Specifically, by
                using contrastive learning to align representations from <strong>pre-trained image and text
                    encoders</strong> (ResNet-18 and DistilBERT), we hypothesize that the resulting multimodal
                representation can approximate the Platonic representation more closely than either of the unimodal
                representations alone.
            </p>

            <h3>Driving Convergence via Alignment</h3>
            <p>
                The PRH suggests that convergence of representations occurs naturally due to factors such as:
            </p>
            <ul>
                <li>Training on diverse, multi-task datasets.</li>
                <li>Increasingly performant and large-scale model architectures.</li>
                <li>Utilization of multimodal data during training.</li>
            </ul>
            <p>
                Our work extends this hypothesis by proposing an <strong>explicit algorithmic mechanism</strong> for
                driving representational convergence: aligning pre-trained unimodal representations using
                <strong>contrastive learning</strong>. Instead of relying on joint training of large multimodal models,
                our approach demonstrates that:
            </p>
            <ul>
                <li>Lightweight linear and MLP adapters can effectively align frozen unimodal encoders.</li>
                <li>Contrastive alignment in a shared latent space can achieve meaningful convergence toward the
                    Platonic representation.</li>
            </ul>

            <p>
                This approach offers a more efficient alternative to end-to-end multimodal training. By aligning
                representations post hoc, we enable <strong>modular scalability</strong>, where new modalities can be
                incorporated without retraining the entire model.
            </p>

            <h3>Empirical Insights</h3>
            <p>
                Our results provide partial validation of our hypotheses. Specifically, we observed that the
                <strong>aligned multimodal representations</strong> exhibit:
            </p>
            <ul>
                <li><strong>Improved Similarity</strong> to DINOv2-small embeddings compared to the average of unimodal
                    representations.</li>
                <li>A <strong>Cross-Modality Benefit</strong> where text representations, which DINOv2 was not trained
                    to process, became more similar to the image representations.</li>
            </ul>

            <h3>Implications and Future Directions</h3>
            <p>
                Our findings suggest that multimodal alignment via contrastive learning is a promising direction for
                achieving efficient and scalable multimodal representation learning. This work also opens up several
                avenues for further research:
            </p>
            <ul>
                <li><strong>Incorporating Additional Modalities:</strong> Expanding the framework to include audio,
                    video, or graph data could further test the robustness of the alignment mechanism.</li>
                <li><strong>Downstream Task Evaluation:</strong> Assessing the utility of aligned representations on
                    tasks like visual question answering, retrieval, or classification.</li>
                <li><strong>Mechanistic Analysis:</strong> Investigating the internal mechanisms of convergence and how
                    specific adapter architectures influence the alignment process.</li>
            </ul>

            <p>
                Ultimately, this work supports the broader goal of understanding how different modalities can be
                integrated to form a cohesive, universal representation of the world — a pursuit that lies at the heart
                of the Platonic Representation Hypothesis.
            </p>
        </section>

        <section id="future-directions">
            <h2>Future Directions</h2>
            <ul>
                <li>
                    Scale experiments to richer multimodal datasets and benchmarks. Specifically,
                    expanding beyond images and text, we will integrate audio as a third modality.
                    We would Test whether incorporating a third modality firther improvef the kernel alignment metrics
                    beyond what we saw wih two modalities.
                    <p>
                        Same approach as this project but now the dataset would the VGGSound dataset andwe could use
                        something like Wav2Vec as our pre-trained frozen audio encoder.
                        Potential limitaiton is VGGsound doesn't seem straightforward to download this dataset. Each
                        line in the csv file has columns defined by:
                        # YouTube ID, start seconds, label, train/test split.
                    <p>
                    </p>What does the loss look like with three modalites, one may ask?
                    We need a generalization from dual-encoder contrastive loss to any number of modalities.
                    That answer be as simple as generalizing the dot/inner product in the loss (the logits in the
                    exponent)
                    to the <a href="https://math.stackexchange.com/questions/1137102/dot-product-for-3-vectors">Hadamard
                        product</a>
                    of the embeddings of the modalities.
                    </p>
                </li>
                <li>Investigate cross-architecture generalization of aligned embeddings.</li>
                <li>Analyze mechanistic insights into alignment-driven representation convergence.</li>
                <li>Replacing DINOv2-small's feature extractor with our aligned representation in-front of
                    <strong>pre-trained
                        heads</strong>
                    (e.g., for ImageNet classification; see all pretrained heads at the <a
                        href="https://github.com/facebookresearch/dinov2?tab=readme-ov-file">DINOv2 GitHub
                        page</a>).
                    <p>
                        This is related but slightly from what we do in Experiment 2, which was to
                        see how good our aligned multimodal representation is tabula rasa for downstream finetuning
                        tasks as
                        compared to the DINOv2-small features. In Experiment 2 we trained a linear classifier on top of
                        the
                        features extracted from the aligned multimodal representation (keeping those frozen) for a few
                        epochs.
                        In this extension, there would be no further training/finetuning at all; we just want to see how
                        good
                        the representations are "out of the box" with the pretrained heads.
                    </p>
                </li>
                <li>Using all five captions in the Flickr30k dataset as positive anchors for each image instead of just
                    using the first caption as we did here.
                    <p>
                        "It's been said that an image is worth a thousand words. In the Fickr30k dataset, an image is
                        worth 5 captions." However in our formulation in experiment 1a we opted to use just a single
                        caption (the first one) for each datum to use the simple contrastive loss formulation. We
                        experiment with using all 5 captions as positive anchors for each image . Specifically for each
                        datum \( (i) \), instead of a pair \( (x^{(i)}, y^{(i)}) \), we would now have a sixtuplet
                        \( (x^{(i)}, y_1^{(i)}, y_2^{(i)}, \dots, y_5^{(i)}) \) where \( y_1^{(i)}, \dots, y_5^{(i)} \)
                        are
                        the text embeddings for each of the captions corresponding to the \(i\)-th image. The dot
                        product terms in our loss function would be replaced by this average:
                        \( \frac{1}{5\tau} \sum_{k=1}^5 f_\text{img}(x^{(i)}) \cdot
                        g_\text{text}(y_k^{(j)}) \)
                    </p>
                </li>
            </ul>
        </section>

    </main>

    <footer>
        <p> Quilee Simeon & Gabe Manso | MIT Deep Learning Final Project Blog | December 2024</p>
    </footer>
</body>


<style>
    figure {
        display: flex;
        /* Arrange images side by side */
        flex-direction: column;
        /* Ensure caption appears below images */
        align-items: center;
        /* Center the content */
        gap: 10px;
        /* Add space between images and caption */
    }

    .figures-container {
        display: flex;
        /* Arrange figures side by side */
        justify-content: center;
        /* Center the figures on the page */
        gap: 20px;
        /* Add spacing between the figures */
    }

    figure img {
        width: 400px;
        /* Adjust image width */
        height: auto;
        /* Maintain aspect ratio */
    }

    figcaption {
        text-align: center;
        /* Center-align the caption text */
        font-size: 14px;
        /* Adjust caption font size */
        color: #555;
        /* Add a softer color for the caption */
    }
</style>

</html>